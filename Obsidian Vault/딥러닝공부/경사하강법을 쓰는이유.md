

> **“왜 미분이 0이 되는 지점을 바로 계산하지 않고, 반복(iteration)으로 조금씩 가는가?”**

---

## 1. 이상적으로는 맞아요

수학적으로는  
함수 $L(w)$ 의 최소값을 구할 때  
$$\frac{dL}{dw} = 0$$  
을 풀면 됩니다.  
이게 바로 **정확한 최적해(closed-form solution)** 이죠.

예를 들어,  
$$L(w) = (w - 3)^2$$  
라면,  
$$\frac{dL}{dw} = 2(w - 3) = 0$$  
→ $w = 3$ 이 바로 최소점이에요.

---

## 2. 그런데 **신경망의 손실함수는 단순하지 않아요**

딥러닝에서의 $L(w)$ 는 이런 단순한 2차식이 아니라  
**수백만 개의 파라미터**를 가진 **비선형 함수**입니다.

예를 들어,

- $w$ 는 10⁶개 이상 (CNN, Transformer 등)
    
- $L(w)$ 는 **비선형 + 다층 합성 함수** (sigmoid, ReLU, softmax 등)
    
- **미분이 0인 지점을 방정식으로 풀 수 없음**
    

즉,  
$$\nabla L(w) = 0$$  
을 **직접 풀 수가 없어요.**  
이건 거의 **10⁶차 방정식**을 푸는 것과 같아요.

---

## 3. 그래서 등장한 방법이 **Gradient Descent (경사하강법)**

해결 아이디어는 간단합니다:

> “정확히 풀 수 없으니, 기울기를 따라 조금씩 내려가 보자.”

수식으로는  
$$  
w_{k+1} = w_k - \eta \nabla L(w_k)  
$$

- $\nabla L(w_k)$ : 현재 위치에서의 기울기(경사)
    
- $\eta$ : learning rate (한 번에 이동할 거리)
    
- 반복(iteration)하며 점점 $L$이 작아지는 방향으로 이동
    

이렇게 하면,  
언젠가 $\nabla L(w_k)$ ≈ 0 인 근처,  
즉 **지역 최소점(local minimum)** 에 도달합니다.

---

## 4. 즉, 반복(iteration)은 “계산 불가능성”의 현실적 대안

|접근|설명|가능 여부|
|---|---|---|
|미분=0으로 직접 풀이|수식으로 최소점 해석적 계산|딥러닝에서는 불가능|
|Gradient Descent|수치적으로 점진적 접근|현실적, GPU로 병렬 계산 가능|

---

## 5. 또 하나의 이유 — 데이터 단위가 너무 큼

딥러닝은 **전체 데이터셋**을 한 번에 쓰면 연산량이 폭발하므로,  
데이터를 **mini-batch 단위**로 나눠서 gradient를 계산합니다.  
즉, 한 번의 iteration은 “데이터 일부로 근사된 gradient”입니다.

그래서 전체 과정이 이렇게 됩니다:

```
for epoch in range(EPOCH):
    for X_batch, y_batch in dataloader:
        loss = L(w_k, X_batch)
        grad = ∂L/∂w
        w_{k+1} = w_k - η * grad
```

---

요약하자면:

- 미분=0 으로 푸는 건 원리상 맞지만, 실제로는 너무 복잡해서 불가능
    
- 그래서 **경사하강법으로 근사**하면서
    
- **반복(iteration)** 을 통해 천천히 최소점 근처로 수렴
    

---


|항목|생물학적 신경|인공신경망 대응|
|---|---|---|
|시냅스 결합 강화|Hebbian learning|Gradient descent|
|발화 조건|threshold potential|activation function|
|전위합산|EPSP/IPSP 합산|weighted sum|
|반복 학습|경험 반복|iteration per batch|
|구조적 학습|시냅스 가소성|parameter update|

---

요약하자면,

> 네, iteration을 통해 학습하는 개념은 **‘시냅스 연결 강도를 반복적으로 조정한다’는 생물학적 뇌의 학습 메커니즘을 수학적으로 단순화한 것**이에요.