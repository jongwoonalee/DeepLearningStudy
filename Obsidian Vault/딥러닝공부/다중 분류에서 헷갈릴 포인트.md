

---

## 📌 다중 분류에서 헷갈릴 포인트

### 1. **Sigmoid vs Softmax**

- **Sigmoid**: 각 클래스별 확률을 독립적으로 계산 → 확률의 합이 1이 아님.
    
    - 예: multi-label classification (여러 라벨 동시 가능)에서 사용
        
- **Softmax**: 모든 클래스의 확률을 합쳐 1이 되도록 정규화 → 하나의 클래스만 선택하는 multi-class classification에 적합.
    
    - 예: 강아지/고양이/소 중 하나 선택
        

👉 따라서 강아지/고양이/소 같은 경우엔 softmax 필수.

---

### 2. **One-hot encoding과 label 표현**

- 라벨이 `[1,0,0]`, `[0,1,0]`, `[0,0,1]` 이런 식으로 되어 있음
    
- 모델 출력이 `[p1, p2, p3]` 확률 분포가 되도록 설계해야 함
    
- BCE(Binary Cross Entropy)랑 헷갈리면 안 됨 → BCE는 두 클래스만 있을 때(0/1).
    

---

### 3. **Cross-Entropy와 Entropy**

- 슬라이드에 나온 식:  
    L=−∑ylog⁡qL = -\sum y \log q
    
- 여기서 $y$는 라벨(one-hot), $q$는 softmax로 나온 확률 분포
    
- **항상 Entropy ≤ Cross-Entropy** 라는 성질 있음 → 모델이 라벨 분포에 가까워질수록 CE ↓
    

---

### 4. **MLE (Maximum Likelihood Estimation) 연결**

- MLE로 생각하면, 확률 분포 $q$가 실제 라벨 분포 $y$에 가까워지도록 likelihood를 최대화 →
    
- 로그를 취하면 cross-entropy loss와 같아짐
    
- 즉, CE loss = MLE의 음의 로그 가능도
    

---

### 5. **Multinomial Logistic Regression**

- 다중 클래스 분류는 사실상 softmax regression = multinomial logistic regression
    
- logit(선형 결합 결과)을 softmax로 변환 → 확률로 바꿔서 cross-entropy와 비교
    

---

## ✅ 정리하면, 헷갈리는 핵심 포인트

1. **왜 softmax 쓰는가?**  
    → 클래스 확률을 합쳐 1로 만들고자 하기 때문
    
2. **sigmoid는 언제 쓰는가?**  
    → 클래스 간 독립적인 multi-label (ex. 한 이미지에 개+고양이 동시 가능)
    
3. **CE loss와 MLE 관계**  
    → CE는 likelihood 최대화와 동일한 원리
    
4. **One-hot과 CE 결합**  
    → one-hot이니까 실제로는 정답 클래스에 해당하는 $- \log q_{정답}$만 loss에 들어감
    

---

## 기호와 표기 풀기

### 데이터·모델 표기

- $D={(x_{n},y_{n})}_{n=1}^{N}$: 샘플 $N$개짜리 데이터셋.
    
- $K$: 클래스 개수.
    
- $x_{n}$: $n$번째 입력.
    
- $y_{n}$: $n$번째 Truth 라벨.
    
    - 인덱스 표기: $y_{n}\in{1,\dots,K}$.
        
    - one-hot 표기: $y_{n}=(y_{n,1},\dots,y_{n,K})$, 단 하나만 1.
        
- $\theta$: 모델 파라미터(가중치와 편향 전부).
    
- $q_{\theta}(y\mid x)$: 입력이 $x$일 때 모델이 예측한 확률분포(Predicted).
    
    - 다중분류라면 각 클래스 확률은  
         $q_{\theta}(y=i\mid x)=\frac{\exp(z_{i}(x;\theta))}{\sum_{j=1}^{K}\exp(z_{j}(x;\theta))}$
        여기서 $z_{i}(x;\theta)$는 클래스 $i$의 점수(softmax 이전 값).
        

> 문헌에서 $p$는 보통 **진짜 분포(Truth)** 에, $q$는 **모델 분포(Predicted)** 에 쓰는 경우가 많습니다. 그래서 $q_{\theta}$로 표기했습니다. 같은 $\theta$를 공유하는 **하나의 모델**이 모든 샘플에 사용되므로 기호가 매번 동일하게 $q_{\theta}$로 반복됩니다. 입력 $x_{n}$과 라벨 $y_{n}$만 샘플마다 달라집니다.

### “큰 파이” 기호

- $\prod_{n=1}^{N} a_{n}$
    대문자 파이 $\prod$는 **곱(product)** 을 뜻합니다.  
    $\prod_{n=1}^{N} q_{\theta}(y_{n}\mid x_{n})=q_{\theta}(y_{1}\mid x_{1})\cdot q_{\theta}(y_{2}\mid x_{2})\cdots q_{\theta}(y_{N}\mid x_{N})$
    
- 왜 곱인가? 보통 샘플들이 i.i.d.라고 가정하기 때문입니다. 독립이면 공동확률은 개별확률의 곱입니다.
    

### Likelihood, Log-likelihood, NLL

- Likelihood(가능도):  
     $L(\theta)=\prod_{n=1}^{N} q_{\theta}(y_{n}\mid x_{n})$
    
- Log-likelihood:  
    $\ell(\theta)=\sum_{n=1}^{N}\log q_{\theta}(y_{n}\mid x_{n})$
    
- Negative Log-likelihood(NLL) = Loss:  
    $\mathrm{NLL}(\theta)=-\sum_{n=1}^{N}\log q_{\theta}(y_{n}\mid x_{n})$
    

one-hot을 쓰면 각 샘플에서 정답 클래스 $y_{n}$에 해당하는 항만 남아  
$\mathrm{NLL}(\theta)=-\sum_{n=1}^{N}\log q_{\theta}(y=y_{n}\mid x_{n})$
이것이 바로 softmax 분류에서 쓰는 **Cross-Entropy** 입니다.

## 왜 로그를 쓰는가

1. **곱을 합으로 바꾸기**  
    $\log$는 단조증가이므로 최대화 대상은 같고, 계산은  
    $\log\prod_{n} q_{\theta}=\sum_{n}\log q_{\theta}$ 
    로 단순해집니다. 합이면 미분·최적화가 쉬워집니다.
    
2. **수치 안정성**  
    확률을 $N$번 곱하면 매우 작아집니다. 예컨대 $0.9^{60000}=e^{60000\log 0.9}\approx e^{-6310}\approx 0$로 언더플로우가 납니다. 로그를 취하면 합으로 바뀌어 안정적입니다.
    
3. **이론적 일치성**  
    $H(p,q)=H(p)+KL(p\Vert q)$  
    분류에서 Truth 분포 $p$가 one-hot이면 $H(p)$는 상수이므로, **CE 최소화 = $KL(p\Vert q)$ 최소화** 와 동치가 됩니다. 즉 모델 분포 $q_{\theta}$를 Truth 분포 $p$에 가깝게 만듭니다.
    
4. **볼록성/미분 용이성**  
    선형 점수에 softmax를 붙인 모델에서 로그가능도는 잘 알려진 좋은 최적화 성질을 가집니다.
    

## 인덱스 표기와 one-hot 표기 모두 보기

- 인덱스 라벨일 때  
    $\mathrm{CE}=-\sum_{n=1}^{N}\log q_{\theta}(y=y_{n}\mid x_{n})$
    
- one-hot 라벨일 때  
    $\mathrm{CE}=-\sum_{n=1}^{N}\sum_{i=1}^{K} y_{n,i}\log q_{\theta}(y=i\mid x_{n})$
    one-hot이므로 실제로는 정답 클래스 항 하나만 남습니다.
    

## 아주 작은 수치 예

샘플 3개, 각 Truth 클래스에 대한 Predicted 확률이 $0.9, 0.7, 0.2$이면

- Likelihood $=0.9\times0.7\times0.2=0.126$
    
- NLL $=-(\log 0.9+\log 0.7+\log 0.2)\approx 2.072$
    

여기서 세 확률 중 하나를 키우면 Likelihood는 커지고 NLL은 줄어듭니다(목표와 일관).

## i.i.d. 가정에 대한 한 줄

샘플들이 독립이 아니거나 동일분포가 아니면 $\prod_{n}$ 형태가 달라질 수 있습니다. 보통의 분류 세팅에서는 i.i.d. 가정을 씁니다.

## 엔트로피는 왜 plogq 냐?
좋은 포인트예요 👍 “왜 굳이 로그에 $p$를 곱해서 평균을 내냐?”가 처음에 가장 헷갈리는 부분이거든요. 차근차근 설명드릴게요.

---

## 1. 한 사건의 정보량 = $-\log p$

- 직관: 희귀한 사건일수록(확률 $p$가 작을수록) “정보량”이 크다.
    
- 정의: 사건 $i$가 일어났을 때의 정보량
    
    $I(i) = -\log p_{i}$
- 예시:
    
    - 동전 앞면 ($p=0.5$) → $I=-\log(0.5)=1$ bit
        
    - 주사위에서 특정 숫자 ($p=1/6$) → $I=-\log(1/6)\approx 2.585$ bits
        

즉, **$-\log p$는 사건 1회에 담긴 정보량**입니다.

---

## 2. 평균 정보량 = 엔트로피

하지만 실제로는 사건이 여러 확률로 발생합니다.

- 앞면($p=0.5$), 뒷면($p=0.5$) 같이 확률이 다름.
    
- 따라서 “평균적으로 1회 사건이 담는 정보량”을 계산해야 함.
    

그래서 확률 $p_{i}$를 가중치로 곱해서 평균을 냅니다:

$H(p) = \sum_{i} p_{i}\,I(i) = -\sum_{i} p_{i}\log p_{i}$

---

## 3. 직관적 해석

- $-\log p_{i}$: 사건 $i$가 발생했을 때의 “코드 길이” (얼마나 긴 비트로 표현해야 하는가)
    
- $p_{i}$: 그 사건이 실제로 발생할 확률
    
- $p_{i} \cdot (-\log p_{i})$: 그 사건의 “기대 정보량 기여분”
    
- 전부 합치면 **평균 코드 길이 = 엔트로피**
    

즉, **$p$를 곱하는 이유는 단순 평균이 아니라, 확률에 따라 가중 평균을 내기 위해서**입니다.

---

## 4. 다시 정리

- $-\log p_{i}$ = 사건 하나의 정보량
    
- $p_{i}$를 곱하는 이유 = 그 사건이 발생할 확률만큼 가중치를 주기 위해서
    
- 합쳐서 평균내면 $H(p)$ = **이 분포의 평균 정보량(불확실성)**
    

---

✅ 한 문장 요약:  
**엔트로피는 “사건별 정보량($-\log p$)”을, 그 사건이 일어날 확률($p$)로 가중 평균한 값**이라서 $p \log p$ 형태가 나오는 겁니다.

---

## 역사적 배경:
로그(logarithm)는 **역사적으로 계산의 효율성을 극대화하고, 컴퓨터 이전의 시대에 수학적 곱셈을 덧셈으로 바꾸기 위해 도입된 도구**였으며,  
현대 컴퓨터나 딥러닝 모델에서도 **정보량 계산, 확률 곱 연산의 안정화, 지수적 성장의 다루기** 등에서 유사한 목적을 위해 사용됩니다.

---

## 🧠 역사적 맥락

### 1. **17세기 로그의 도입**

- John Napier가 1614년 도입
    
- 목적: **곱셈을 덧셈으로 바꾸기 위해**
    
    log⁡(ab)=log⁡a+log⁡b\log(ab) = \log a + \log b
- 당시 수학자/천문학자들은 수기로 계산을 해야 했으므로, 로그는 **계산을 획기적으로 단순화**시킨 도구였습니다.
    

---

## 💻 현대 컴퓨터/딥러닝에서 로그가 여전히 쓰이는 이유

### 1. **곱의 누적 → log-space에서의 덧셈 변환**

- 예: 베이즈 정리에서 조건부 확률이 많아질수록 곱이 복잡해짐
    
- 로그를 취하면:
    
     $\log(p_1 \cdot p_2 \cdot p_3) = \log p_1 + \log p_2 + \log p_3$
- → 연산 효율성 + underflow(소수점 너무 작아져서 0되는 문제) 방지
    

---

### 2. **Cross-Entropy Loss에서의 log**

- 예:
    
     $\text{Loss} = -\sum_{i} y_{i} \log(p_{i})$
    - $y_i$: Truth (one-hot)
        
    - $p_i$: Predicted probability (softmax output)
        
- 이유:
    
    - $p_i$는 0~1 사이 확률 → $\log(p_i)$는 음수
        
    - 잘못 예측할수록 $\log(p_i)$가 작아짐 → **패널티 커짐**
        
    - 잘 예측할수록 $\log(p_i) \to 0$ → **Loss 작아짐**
        

---

### 3. **정보이론적 해석**

- 로그는 **정보량(information content)**의 단위이기도 함:
    
    $I(x) = -\log p(x)$
- 희귀할수록 정보량 큼 → 예측 어려움 → 패널티 큼
    

---

## 🧠 한 줄 요약

> 로그는 **역사적으로는 곱셈 단순화 도구**,  
> 현대에는 **정보량 해석 + 수치 안정성 + 연산 효율성**을 동시에 만족시키는 **수학적 표현의 축약 도구**입니다.

---

# softmax 분류에서 cross-entropy loss가 어떻게 유도되는지 설명하는 핵심 

당신께서 이해하시기 쉽게, 수식의 의미와 용어 하나하나를 차근차근 풀어드리겠습니다.

---

## 🧩 1. 먼저 문장을 분해해봅시다:

> **“one-hot을 쓰면 각 샘플에서 정답 클래스 $y_n$에 해당하는 항만 남아  
> $\mathrm{NLL}(\theta) = -\sum_{n=1}^{N} \log q_{\theta}(y = y_n \mid x_n)$  
> 이것이 바로 softmax 분류에서 쓰는 Cross-Entropy입니다.”**

---

## 🔹 2. 주요 용어 정리

|기호|의미|
|---|---|
|$x_n$|$n$번째 입력 샘플 (예: 이미지, 문장 등)|
|$y_n$|$x_n$의 정답 레이블 (예: 고양이, 개, 자동차 등)|
|$q_{\theta}(y \mid x_n)$|입력 $x_n$에 대해 모델 파라미터 $\theta$가 예측한 확률 분포|
|$q_{\theta}(y = y_n \mid x_n)$|그 중에서도 **정답 클래스 $y_n$일 확률**만 뽑은 것|
|$-\log(...)$|확률을 log로 바꿔서 ‘Loss’를 계산. 잘못 맞출수록 패널티가 큼|
|$\sum_{n=1}^N$|전체 $N$개의 샘플에 대해 평균/합계를 구함|

---



|$n$|$n$번째 데이터 샘플 (예: 전체 배치 중 7번째 이미지)|
|$k$|전체 $C$개의 클래스 중에서 $k$번째 클래스 (예: 개, 고양이, 새 중 "고양이"에 해당)|

## 🔸 3. 왜 One-hot을 쓰면 "그 항만 남는가?"

### 예시로 들어 보겠습니다:

#### 분류 문제 (예: 3-class)

- 정답이 고양이(cat)일 때: $y_n = 1$
    
- one-hot 표현:
    
    $]y_n = \begin{bmatrix} 0 & 1 & 0 \end{bmatrix}$

#### 모델 출력 (softmax 확률):

- $q_\theta(x_n) = \begin{bmatrix} 0.1 & 0.8 & 0.1 \end{bmatrix}$  
    → 모델은 80% 확률로 "고양이"라고 예측함
    

### Cross-Entropy 정의:

$\mathrm{CE}(y_n, q_\theta) = - \sum_{k=1}^{C} y_{n,k} \log q_{\theta,k}(x_n)$

> 여기서 $y_{n,k}$는 one-hot이므로, 정답 위치에만 1이 있고 나머지는 0 →  
> 결국 **“정답인 항만 남고”** 나머지는 0 되므로:

$\mathrm{CE} = -\log q_{\theta}(y = y_n \mid x_n)$

즉,

- 모델이 정답 클래스에 대해 얼마나 확신했는지를 log로 측정
    
- **잘 예측할수록** $\log p \to 0$ → 작은 loss
    
- **틀릴수록** $p \to 0$ → $\log p \to -\infty$ → 큰 loss
    

---

## 🔸 4. 그래서 결국 이렇게 됩니다:

$\mathrm{NLL}(\theta) = - \sum_{n=1}^N \log q_\theta(y = y_n \mid x_n)$

- NLL = Negative Log-Likelihood
    
- softmax를 통과한 확률에서 정답인 것만 뽑아서 log 취한 후 평균
    

그리고 이게 바로 우리가 PyTorch에서 쓰는 cross-entropy와 같은 식입니다:

```python
nn.CrossEntropyLoss()
```

---

## 🔹 5. 요약 구조도

```
[정답 y_n (one-hot)] × [예측 확률 q(y|x)]  →  Cross-Entropy Loss
       ↓                              ↓
   정답 항만 1                         log(p정답)
       ↓                              ↓
   전체 합에서 그 항만 남음         →  -log(p정답)
```

---

## ✅ 최종 정리 문장

> **Cross-Entropy는, softmax 분류에서 "정답 클래스가 나올 확률만 뽑아서 로그를 취한 것"의 평균입니다.**  
> One-hot 벡터 덕분에 정답 항만 남고, 그게 NLL(Negative Log-Likelihood)과 같은 식이 됩니다.

---

필요하시면:

- PyTorch `CrossEntropyLoss` 내부 작동 구조
    
- softmax-log cross-entropy가 왜 곱보다 log가 좋은지도  
    시각화 + 수식으로 정리해드릴 수 있습니다.